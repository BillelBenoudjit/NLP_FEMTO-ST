{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Named entity recognition\n",
        "This notebook is made to train a model for named entity recognition using transformers applied to a public dataset from the DEFT 2020 challenge."
      ],
      "metadata": {
        "id": "K2WoTmFiR4eK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlQ16Dq912gs"
      },
      "source": [
        "## Preparation of the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download needed packages.**"
      ],
      "metadata": {
        "id": "7NS92GZrSOxR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxSWb0aDoH6X",
        "outputId": "5338abb0-ec2c-4cf7-c9ce-b12dc0a6c11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.19.3-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.11.4)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 28.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->transformers[sentencepiece]) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentencepiece\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.19.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 46.4 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.5\n",
            "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 43.9 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, dill, aiohttp, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.13\n",
            "    Uninstalling multiprocess-0.70.13:\n",
            "      Successfully uninstalled multiprocess-0.70.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 multiprocess-0.70.12.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 974 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=22ed47d5af3a7ca39696b215d9a4bc3ddf565e78ba820827f815e2906a3ae06d\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.46.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.21.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Import needed packages\n",
        "!pip install transformers[\"sentencepiece\"]\n",
        "!pip install datasets\n",
        "!pip install seqeval\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-SaHKQAJnQn",
        "outputId": "b8e49cac-4eae-4752-87f2-9f1bd9c803fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHpLbADx2BXA"
      },
      "source": [
        "## Preparation of the data\n",
        "We are using the DEFT dataset downloaded in our local machine (not Google Drive), to be able to use it, we upload it directly to the runtime environment of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Files treatment per set\n",
        "We start by treating the data files by getting preparing files according to the needed annotations."
      ],
      "metadata": {
        "id": "wFKyHYvbTlaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unzip the imported data file (.tar dataset version).**\n"
      ],
      "metadata": {
        "id": "TLzjyfC0SdaC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fG5HZRMoREs"
      },
      "outputs": [],
      "source": [
        "# Unzip Dataset file\n",
        "import tarfile\n",
        "\n",
        "file = tarfile.open(\"DEFT2020-cas-cliniques.tar.gz\")\n",
        "file.extractall(\"./\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unzip the imported data file (.zip dataset version).**"
      ],
      "metadata": {
        "id": "3RzDEJ4WSx_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9RQYTjxnCLK"
      },
      "outputs": [],
      "source": [
        "# Unzip Dataset file\n",
        "\n",
        "import zipfile\n",
        "\n",
        "path_to_zip_files = [\"t3-appr.zip\", \"t3-test.zip\"]\n",
        "\n",
        "directory_to_extract_to = \"./\"\n",
        "\n",
        "for path_to_zip_file in path_to_zip_files:\n",
        "  with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the number of files for the training and test sets.**"
      ],
      "metadata": {
        "id": "4wDPeiQpS5jz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eTm5IBjnW18",
        "outputId": "2e1ad304-1305-4564-dc2a-58bc518060c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of text and annotation files in the directory: t3-appr is: 200\n",
            "The total number of files in the directory: t3-appr is: 204\n",
            "The number of text and annotation files in the directory: t3-test is: 67\n",
            "The total number of files in the directory: t3-test is: 71\n",
            "The number of text and annotation files in the dataset: 167\n"
          ]
        }
      ],
      "source": [
        "# Check the dataset\n",
        "import os\n",
        "import glob\n",
        "\n",
        "directories = [\"t3-appr\", \"t3-test\"]\n",
        "\n",
        "totalNumberOfFiles = 0\n",
        "for directory in directories:\n",
        "  numberOfFiles = 0\n",
        "  # Print dataset files (Text + annotations)\n",
        "  for filename in glob.iglob(f'{directory}/*.txt'):\n",
        "    numberOfFiles += 1\n",
        "    totalNumberOfFiles += 1\n",
        "  for filename in glob.iglob(f'{directory}/*.ann'):\n",
        "    numberOfFiles += 1\n",
        "  print(f'The number of text and annotation files in the directory: {directory} is: {numberOfFiles}')\n",
        "  print(f'The total number of files in the directory: {directory} is: {len(os.listdir(directory))}')\n",
        "\n",
        "print(f'The number of text and annotation files in the dataset: {totalNumberOfFiles}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rewrite the annotation files (to change the annotations) for the training set.**"
      ],
      "metadata": {
        "id": "kxyot2NYTKAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZDyRkzg0I6Y"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "grouped_files = defaultdict(int)\n",
        "EXTENSIONS = {'.ann', '.txt'}\n",
        "\n",
        "for f in os.listdir('t3-appr'):\n",
        "  name, ext = os.path.splitext(os.path.join('t3-appr', f))\n",
        "  if ext in EXTENSIONS:\n",
        "    grouped_files[name] += 1\n",
        "\n",
        "for name in grouped_files:\n",
        "  # with open('{}.ann'.format('DEFT-cas-cliniques/' + name[8:]), \"r\", encoding=\"utf-8\") as file:\n",
        "  with open('{}.ann'.format(name), \"r\", encoding=\"utf-8\") as file:\n",
        "    annotations = file.read()\n",
        "  with open('{}.ann'.format(name), \"w\", encoding=\"utf-8\") as ann_file:\n",
        "    for annotation in annotations.split(\"\\n\"):\n",
        "      temp = annotation.replace(\"\\t\", \" \")\n",
        "      elements = temp.split(\" \", 4)\n",
        "      if 'T' in elements[0]:\n",
        "        if elements[1] not in [\"age\", \"genre\", \"issue\", \"origine\", \"frequence\", \"date\", \"duree\"]:\n",
        "          # 'valeur', 'dose', 'mode', 'substance', 'examen', 'traitement', 'anatomie', 'moment', 'sosy', 'pathologie'\n",
        "          # if elements[1] not in ['valeur', 'dose', 'mode', 'substance', 'examen', 'traitement', 'anatomie', 'moment']:\n",
        "            ann_file.write(annotation + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rewrite the annotation files (to change the annotations) for the test set.**"
      ],
      "metadata": {
        "id": "YT7xYan6TYKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92vKnXPe2bGg"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "grouped_files = defaultdict(int)\n",
        "EXTENSIONS = {'.ann', '.txt'}\n",
        "\n",
        "for f in os.listdir('t3-test'):\n",
        "  name, ext = os.path.splitext(os.path.join('t3-test', f))\n",
        "  if ext in EXTENSIONS:\n",
        "    grouped_files[name] += 1\n",
        "\n",
        "for name in grouped_files:\n",
        "  with open('{}.ann'.format('DEFT-cas-cliniques/' + name[8:]), \"r\", encoding=\"utf-8\") as file:\n",
        "    annotations = file.read()\n",
        "  with open('{}.ann'.format(name), \"w\", encoding=\"utf-8\") as ann_file:\n",
        "    for annotation in annotations.split(\"\\n\"):\n",
        "      temp = annotation.replace(\"\\t\", \" \")\n",
        "      elements = temp.split(\" \", 4)\n",
        "      if 'T' in elements[0]:\n",
        "        if elements[1] not in [\"age\", \"genre\", \"issue\", \"origine\", \"frequence\", \"date\", \"duree\"]:\n",
        "          # 'valeur', 'dose', 'mode', 'substance', 'examen', 'traitement', 'anatomie', 'moment', 'sosy', 'pathologie'\n",
        "          # if elements[1] not in ['valeur', 'dose', 'mode', 'substance', 'examen', 'traitement', 'anatomie', 'moment']:\n",
        "            ann_file.write(annotation + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zip new obtained train data folder.**"
      ],
      "metadata": {
        "id": "iPeMwV4CVCtj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npslFN3v2weY"
      },
      "outputs": [],
      "source": [
        "# !zip -r /content/t3-appr-two_classes.zip /content/t3-appr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zip the new obtained test data folder.**"
      ],
      "metadata": {
        "id": "yP6tTZMrU94k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPrw6IJV23K7"
      },
      "outputs": [],
      "source": [
        "# !zip -r /content/t3-test-two_classes.zip /content/t3-test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preare data per set\n",
        "In this phase, we get convert the data from the brat format to the CoNLL format to make usable by the transformers model."
      ],
      "metadata": {
        "id": "xvpu-_TSTvaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Track the frequency of each annotation.**"
      ],
      "metadata": {
        "id": "URLHrB9KTd4W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9bgkWtlyyUt"
      },
      "outputs": [],
      "source": [
        "annotations_frequency = {\n",
        "    'valeur': 0, \n",
        "    'issue': 0, \n",
        "    'dose': 0, \n",
        "    'mode': 0, \n",
        "    'genre': 0, \n",
        "    'substance': 0, \n",
        "    'origine': 0, \n",
        "    'sosy': 0, \n",
        "    'frequence': 0, \n",
        "    'examen': 0, \n",
        "    'traitement': 0, \n",
        "    'anatomie': 0, \n",
        "    'age': 0, \n",
        "    'date': 0, \n",
        "    'pathologie': 0, \n",
        "    'moment': 0, \n",
        "    'duree': 0, \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieve the text and annotations of a file.**"
      ],
      "metadata": {
        "id": "WEUGDHnET1PH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNzoYKnQ1bGC"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "def load_data(fileName):\n",
        "  with open('{}.txt'.format(fileName), \"r\", encoding=\"utf-8\") as txt_file, \\\n",
        "    open('{}.ann'.format(fileName), \"r\", encoding=\"utf-8\") as ann_file:\n",
        "    # Get text\n",
        "    script = txt_file.read()\n",
        "    text = script.split(\"\\n\\n\\n\\n\")\n",
        "    text[0] = text[0].replace(\".\\n\", \". \").replace(\"\\n\", \" \").strip()    \n",
        "    \n",
        "    annotations = ann_file.read().replace(\"\\t\", \" \")\n",
        "    annotations = annotations[:len(annotations)-1] + \"\" + annotations[len(annotations):]\n",
        "    # print(annotations)\n",
        "    return text, annotations, script"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the tokens of a text.**"
      ],
      "metadata": {
        "id": "SdwAr9QhT7dK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzp0IzfAeSzb"
      },
      "outputs": [],
      "source": [
        "def get_tokens(text):\n",
        "  tokens = text[0].split(\" \")\n",
        "  # tokens = re.findall(r\"\\w+(?:[-]{1,2})?(?:\\w+)?(?:[-]{1,2})?(?:\\w+)?\", str(text))\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove nested annotations by keeping the longest ones only for each list of annotationa of a file.**"
      ],
      "metadata": {
        "id": "VsmWcMrkUNlJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxYL_HqJhSW6"
      },
      "outputs": [],
      "source": [
        "def remove_nested_annotations(annotated_data):\n",
        "  # Remove nested annotations\n",
        "  nested_annotations = 0\n",
        "  text_raws = []\n",
        "  cursors = []\n",
        "  indices = []\n",
        "  # print(\"---------------- remove nested annotations ----------------\")\n",
        "  for row in annotated_data:\n",
        "    # print(row)\n",
        "    start = row[\"positions\"][0]\n",
        "    end = row[\"positions\"][1]\n",
        "    index = annotated_data.index(row)\n",
        "    # print(index)\n",
        "    delete = False\n",
        "    # print(\"cursors = \", cursors)\n",
        "    for cursor in cursors:\n",
        "      if cursor[0] <= start <= cursor[1]:\n",
        "        indices.append(index)\n",
        "        delete = True\n",
        "        break\n",
        "    if not delete:\n",
        "      cursors.append((start, end)) \n",
        "  if len(indices) > 0:\n",
        "    nested_annotations = len(indices) + 1\n",
        "  for i in reversed(indices):\n",
        "    del annotated_data[i]\n",
        "  # print(\"---------------- -------------------------- ----------------\")\n",
        "  return annotated_data, nested_annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Structure annotations to make use of them.**"
      ],
      "metadata": {
        "id": "9vIhB2GdVeIt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ioy9b2Lyedbt"
      },
      "outputs": [],
      "source": [
        "def structure_annotations(annotations):\n",
        "  elements = []\n",
        "  labels = []\n",
        "  positions = []\n",
        "  entities = []\n",
        "  annotated_data = []\n",
        "  nested_annotations = 0\n",
        "  real_annotations = 0\n",
        "  # Go through annotations and for each one get its different parts\n",
        "  for annotation in annotations.split(\"\\n\"):\n",
        "    elements = annotation.split(\" \", 4)\n",
        "    if 'T' in elements[0]:\n",
        "      # \"pathologie\", \"sosy\"\n",
        "      # , \"anatomie\", \"dose\", \"examen\", \"mode\", \"moment\", \"substance\", \"traitement\", \"valeur\"\n",
        "      # if elements[1] in [\"examen\"]:\n",
        "      if elements[1] not in [\"age\", \"genre\", \"issue\", \"origine\", \"frequence\", \"date\", \"duree\"]:\n",
        "      # if len(elements[4].split(\" \")) <= 2 and elements[1] != 'issue':\n",
        "        labels.append(elements[1])\n",
        "        positions.append((int(elements[2]), int(elements[3])))\n",
        "        entities.append(elements[4])\n",
        "        annotations_frequency[elements[1]] += 1\n",
        "        annotated_data.append({\n",
        "          'labels': elements[1],\n",
        "          'positions': (int(elements[2]), int(elements[3])),\n",
        "          'entities': elements[4] \n",
        "        })\n",
        "  annotated_data = sorted(annotated_data, key=lambda x: x['positions'][0])\n",
        "  real_annotations = len(annotated_data)\n",
        "  annotated_data, nested_annotations = remove_nested_annotations(annotated_data)\n",
        "  return annotated_data, nested_annotations, real_annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transform the data to a BIO annotation format for a given text.**"
      ],
      "metadata": {
        "id": "jQ4rzBaxVkuP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOQQpcDsipOp"
      },
      "outputs": [],
      "source": [
        "def annotate_data(text, annotated_data):\n",
        "  # Annotate text\n",
        "  cursor = 0\n",
        "  token_docs = []\n",
        "  label_docs = []\n",
        "  not_annotated = 0\n",
        "  annotated = 0\n",
        "  for row in annotated_data:\n",
        "    start = row[\"positions\"][0]\n",
        "    end = row[\"positions\"][1]\n",
        "    if cursor != start:\n",
        "      not_annotated += 1\n",
        "      for token in text[0][cursor:start].split(\" \"):\n",
        "        if len(token) != 0:\n",
        "          token_docs.append(token)\n",
        "          label_docs.append('O')\n",
        "    order = 0\n",
        "    annotated += 1\n",
        "    for token in row[\"entities\"].split(\" \"):\n",
        "      token_docs.append(token)\n",
        "      if order == 0:\n",
        "        label_docs.append(f'B-{row[\"labels\"]}')\n",
        "      else:\n",
        "        label_docs.append(f'I-{row[\"labels\"]}')\n",
        "      order += 1\n",
        "    # annotations_backup[row[\"labels\"]].append(row[\"entities\"])\n",
        "    cursor = end\n",
        "\n",
        "  if cursor != len(text[0]):\n",
        "    not_annotated += 1\n",
        "    for token in text[0][cursor:].split(\" \"):\n",
        "      if len(token) != 0:\n",
        "        token_docs.append(token)\n",
        "        label_docs.append('O')\n",
        "  return token_docs, label_docs, annotated, not_annotated"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load NLTK tokenizer to use it in the text division into sentences.**"
      ],
      "metadata": {
        "id": "3rDbIONzVx5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMguYOcSJsh0"
      },
      "outputs": [],
      "source": [
        "# Load NLTK tokenizer\n",
        "nltk_tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a BIO annotated dataset (with text splited into sentences) from a list of files.**"
      ],
      "metadata": {
        "id": "HSt8ddYrWB6L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAgF-_KdoJzO"
      },
      "outputs": [],
      "source": [
        "def get_data(grouped_files):\n",
        "  number_of_annotations = 0\n",
        "  cpt = 0\n",
        "  numberRealAnnotations = 0\n",
        "  numberOfAnnotations = 0\n",
        "  numberNotAnnotations = 0\n",
        "\n",
        "  file_names = []\n",
        "  token_docs = []\n",
        "  label_docs = []\n",
        "  nested = []\n",
        "\n",
        "  numberOfDocs = 0\n",
        "  numberOfSentences = 0\n",
        "  # Iterate through files to get data\n",
        "  for name in grouped_files:\n",
        "    # print(name)\n",
        "    \n",
        "    if grouped_files[name] == len(EXTENSIONS): \n",
        "      file_names.append(name)\n",
        "    \n",
        "    text = [] # Text of one file\n",
        "    annotations = [] # Annotations of one file\n",
        "\n",
        "    # Get the text and annotations\n",
        "    text, annotations, script = load_data(name)\n",
        "    \n",
        "    # Get tokens from text\n",
        "    tokens = get_tokens(text)\n",
        "    \n",
        "    # # Structure annotations and remove the nested ones\n",
        "    annotated_data, nested_annotations, real_annotations = structure_annotations(annotations)\n",
        "    number_of_annotations += len(annotated_data)\n",
        "\n",
        "    if nested_annotations != 0:\n",
        "      nested.append(nested_annotations)\n",
        "\n",
        "    cpt += nested_annotations\n",
        "    numberRealAnnotations += real_annotations\n",
        "    \n",
        "    # # Annotate data\n",
        "    temp = 0\n",
        "    tokens, labels, annotated, not_annotated = annotate_data(text, annotated_data)\n",
        "\n",
        "    numberOfAnnotations += annotated\n",
        "    numberNotAnnotations += not_annotated\n",
        "\n",
        "    sentences = []\n",
        "    script_sentences = nltk_tokenizer.tokenize(script)\n",
        "    for sentence in script_sentences:\n",
        "      if \"\\n\" in sentence:\n",
        "        for s in sentence.split(\"\\n\"):\n",
        "          if len(s) != 0:\n",
        "            sentences.append(s)\n",
        "      else:\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    # # Save the processed text and annotations \n",
        "    if labels.count('O') != len(labels):\n",
        "      token_docs.append(tokens)\n",
        "      label_docs.append(labels)\n",
        "      # cpt = 0\n",
        "      # if numberOfDocs < 100:\n",
        "      #   numberOfSentences += len(sentences)\n",
        "      # numberOfDocs += 1\n",
        "      # for sentence in sentences:\n",
        "      #   i = len(sentence.split(\" \")) - 1\n",
        "      #   temp_tokens = []\n",
        "      #   temp_labels = []\n",
        "      #   while i < len(sentence) and cpt < len(tokens):\n",
        "      #     i += len(tokens[cpt])\n",
        "      #     temp_tokens.append(tokens[cpt])\n",
        "      #     temp_labels.append(labels[cpt])\n",
        "      #     cpt += 1\n",
        "      #   token_docs.append(temp_tokens)\n",
        "      #   label_docs.append(temp_labels)\n",
        "      \n",
        "  print(\"Number of nested annotations =\", cpt)\n",
        "  print(\"Number of real annotations =\", numberRealAnnotations)\n",
        "  print(\"------------------------------------------------------------------------\")\n",
        "  print(\"Number of annotated entities =\", numberOfAnnotations)\n",
        "  print(\"Number of not annotated entities =\", numberNotAnnotations)\n",
        "  print(\"Number of annotations =\", number_of_annotations)\n",
        "  print(\"------------------------------------------------------------------------\")\n",
        "  print(\"Number of sentences (tokens) =\", len(token_docs))\n",
        "  print(\"Number of sentences (labels) =\", len(label_docs))\n",
        "  print(\"------------------------------------------------------------------------\")\n",
        "  print(\"Number of docs =\", numberOfDocs)\n",
        "  print(\"Number of sentences =\", numberOfSentences)\n",
        "  print(\"------------------------------------------------------------------------\")\n",
        "  print(\"nested =\", nested)\n",
        "  return token_docs, label_docs, nested"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the list of tokens and respective labels per set.**"
      ],
      "metadata": {
        "id": "Jvid2IfjWgPl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB4hjJs8-Jte",
        "outputId": "28d87503-1f4e-4bb2-f813-e16f366e55ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nested annotations = 2112\n",
            "Number of real annotations = 7844\n",
            "------------------------------------------------------------------------\n",
            "Number of annotated entities = 5832\n",
            "Number of not annotated entities = 5931\n",
            "Number of annotations = 5832\n",
            "------------------------------------------------------------------------\n",
            "Number of sentences (tokens) = 100\n",
            "Number of sentences (labels) = 100\n",
            "------------------------------------------------------------------------\n",
            "Number of docs = 0\n",
            "Number of sentences = 0\n",
            "------------------------------------------------------------------------\n",
            "nested = [14, 33, 36, 28, 9, 20, 20, 5, 16, 37, 26, 19, 9, 42, 22, 30, 20, 11, 5, 4, 5, 23, 26, 40, 24, 24, 9, 35, 12, 19, 19, 7, 34, 9, 17, 20, 20, 30, 7, 41, 22, 63, 19, 13, 23, 17, 42, 41, 42, 21, 19, 19, 3, 18, 5, 19, 14, 24, 11, 13, 13, 18, 26, 2, 11, 4, 17, 25, 16, 18, 16, 21, 17, 23, 9, 24, 18, 11, 12, 24, 3, 14, 26, 24, 17, 58, 13, 27, 20, 30, 13, 38, 21, 31, 59, 21, 13, 25, 12, 47]\n",
            "Number of nested annotations = 1684\n",
            "Number of real annotations = 4738\n",
            "------------------------------------------------------------------------\n",
            "Number of annotated entities = 3119\n",
            "Number of not annotated entities = 3185\n",
            "Number of annotations = 3119\n",
            "------------------------------------------------------------------------\n",
            "Number of sentences (tokens) = 67\n",
            "Number of sentences (labels) = 67\n",
            "------------------------------------------------------------------------\n",
            "Number of docs = 0\n",
            "Number of sentences = 0\n",
            "------------------------------------------------------------------------\n",
            "nested = [20, 32, 37, 65, 19, 45, 31, 23, 16, 2, 39, 26, 8, 14, 10, 60, 12, 22, 26, 36, 41, 25, 20, 18, 11, 50, 35, 14, 43, 30, 18, 29, 14, 9, 59, 7, 20, 7, 41, 11, 25, 52, 33, 31, 21, 16, 19, 19, 13, 25, 26, 38, 37, 28, 23, 21, 23, 21, 20, 19, 37, 13, 41, 19, 19]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import pprint\n",
        "\n",
        "EXTENSIONS = {'.ann', '.txt'}\n",
        "directories = [\"t3-appr\", \"t3-test\"]\n",
        "train_grouped_files = defaultdict(int)\n",
        "test_grouped_files = defaultdict(int)\n",
        "\n",
        "train_tokens = []\n",
        "train_labels = []\n",
        "test_tokens = []\n",
        "test_labels = []\n",
        "\n",
        "train_nested = []\n",
        "test_nested = []\n",
        "\n",
        "# Get the number of files for one file name\n",
        "for f in os.listdir(directories[0]):\n",
        "  name, ext = os.path.splitext(os.path.join(directories[0], f))\n",
        "  if ext in EXTENSIONS:\n",
        "    train_grouped_files[name] += 1\n",
        "\n",
        "train_tokens, train_labels, train_nested = get_data(train_grouped_files)\n",
        "\n",
        "# Get the number of files for one file name\n",
        "for f in os.listdir(directories[1]):\n",
        "  name, ext = os.path.splitext(os.path.join(directories[1], f))\n",
        "  if ext in EXTENSIONS:\n",
        "    test_grouped_files[name] += 1\n",
        "\n",
        "test_tokens, test_labels, test_nested = get_data(test_grouped_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save obtained test data in .tsv file (BIO format).**"
      ],
      "metadata": {
        "id": "Hpi9UZLJXg4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4B6fmk07vnMA"
      },
      "outputs": [],
      "source": [
        "# save data in IOB format\n",
        "import csv\n",
        "\n",
        "with open('DEFT_test_2_classes_records_sentences.tsv', 'w', newline='\\n', encoding=\"utf-8\") as tsvfile:\n",
        "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
        "    for tokens, labels in zip(test_tokens, test_labels):\n",
        "      for token, label in zip(tokens, labels):\n",
        "        writer.writerow([token, label])\n",
        "      writer.writerow([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUKoul-S2OPW"
      },
      "source": [
        "## Pre-processing of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stats from the datasets"
      ],
      "metadata": {
        "id": "rR_4ve4YXBl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the frequency of each annotation.**"
      ],
      "metadata": {
        "id": "_ELmENURXZif"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_3b4Xyh4zjFN"
      },
      "outputs": [],
      "source": [
        "annotations_frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRx93sEwf-Ok"
      },
      "source": [
        "**Check the tokens and labels count for the training set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YJbLX5hr0Fpi"
      },
      "outputs": [],
      "source": [
        "# Check the number of tokens and labels in the corpus\n",
        "tokens_count = 0\n",
        "labels_count = 0\n",
        "for tokens, labels in zip(train_tokens, train_labels):\n",
        "  tokens_count += len(tokens)\n",
        "  labels_count += len(labels)\n",
        "\n",
        "print(tokens_count)\n",
        "print(labels_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize the distribution of sentences by their length.**"
      ],
      "metadata": {
        "id": "rCDblEk5X4yK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T8FjqOz70GuS"
      },
      "outputs": [],
      "source": [
        "#Lets visualize how the sentences are distributed by their length\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.hist([len(tokens) for tokens in train_tokens], bins=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV-sFvMz2Tr0"
      },
      "source": [
        "### Map labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map labels to numbers and vice-versa in order to be able to use them by the transformers model."
      ],
      "metadata": {
        "id": "NvL1URznYM6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LRIvlWyA8MnF"
      },
      "outputs": [],
      "source": [
        "# Define the labels mapping\n",
        "unique_labels = list(set(label for doc in train_labels for label in doc))\n",
        "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
        "id2label = {id: label for label, id in label2id.items()}\n",
        "\n",
        "print(label2id)\n",
        "print(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlFFxw8e2Y97"
      },
      "source": [
        "### Split data **(data already divided - section commented)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split data (80% train, 20% test).**"
      ],
      "metadata": {
        "id": "2Dv7R1FwYfy6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8GSrqy2734-Z"
      },
      "outputs": [],
      "source": [
        "# # Split data into text and test sets\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train_texts, test_texts, train_labels, test_labels = train_test_split(token_docs, label_docs, test_size=.2, random_state=42)\n",
        "\n",
        "# # Print lengths of each part of the data\n",
        "# print(len(train_texts), len(train_labels))\n",
        "# print(len(test_texts), len(test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the used model"
      ],
      "metadata": {
        "id": "ElJW1VUya3Lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the used model to be used for encoding the data and fine-tuning the named entity recognition model.**"
      ],
      "metadata": {
        "id": "jbovyWhea-PP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6uvX7tPu_Mla"
      },
      "outputs": [],
      "source": [
        "# Define the model id\n",
        "\n",
        "# model_id = 'camembert/camembert-large'\n",
        "model_id = 'camembert-base'\n",
        "# model_id = \"bert-base-cased\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h1K5h-v2mGR"
      },
      "source": [
        "### Encode data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the tokenizer.**"
      ],
      "metadata": {
        "id": "RKP3P8IdbUfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6pmgRI4JAikj"
      },
      "outputs": [],
      "source": [
        "# Import a tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, model_max_length=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3uMa8gZ29_v"
      },
      "source": [
        "**Encode the tokens for the training and test sets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YfLF8fu6i0-u"
      },
      "outputs": [],
      "source": [
        "# Encode text tokens\n",
        "train_text_encodings = tokenizer(\n",
        "    train_tokens, is_split_into_words=True, \n",
        "    return_offsets_mapping=True, padding=\"max_length\", \n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "test_text_encodings = tokenizer(\n",
        "    test_tokens, is_split_into_words=True, \n",
        "    return_offsets_mapping=True, padding=\"max_length\", \n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpBa7HSv3C7x"
      },
      "source": [
        "**Encode labels for each list of encoded tokens.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XDiN7F5oEyCK"
      },
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "import numpy as np\n",
        "\n",
        "def encode_labels(tags, encodings, position):\n",
        "  tags_list = [label2id[label] for label in tags[position]] \n",
        "  # Create an empty array of -100 of length max_length\n",
        "  encoded_labels = np.ones(len(encodings[\"offset_mapping\"][position]), dtype=int) * -100\n",
        "  # Set only labels whose first offset position is 0 and the second is not 0\n",
        "  i = 0\n",
        "  for idx, mapping in enumerate(encodings[\"offset_mapping\"][position]):\n",
        "    if mapping[0] == 0 and mapping[1] != 0 and i < len(tags_list):\n",
        "      # Overwrite label\n",
        "      encoded_labels[idx] = tags_list[i]\n",
        "      i += 1\n",
        "  return encoded_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encode training labels.**"
      ],
      "metadata": {
        "id": "BRQrnVUWbvYy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0GpGfqIszVhi"
      },
      "outputs": [],
      "source": [
        "# Encode labels of the training set\n",
        "train_label_encodings = []\n",
        "for position in range(len(train_labels)):\n",
        "  train_label_encodings.append(encode_labels(train_labels, train_text_encodings, position))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encode test labels.**"
      ],
      "metadata": {
        "id": "7W8TWiOOb2Lj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t26akKJKzb0p"
      },
      "outputs": [],
      "source": [
        "# Encode labels of the test set\n",
        "test_label_encodings = []\n",
        "for position in range(len(test_labels)):\n",
        "  test_label_encodings.append(encode_labels(test_labels, test_text_encodings, position))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1VbILFT3HH_"
      },
      "source": [
        "### Create training and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the features for the training and test sets.**"
      ],
      "metadata": {
        "id": "0H6gxFzwcL42"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1QNssuf1CKgY"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import pandas as pd\n",
        "\n",
        "annotated_train_data = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
        "annotated_test_data = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
        "\n",
        "position = 0\n",
        "for labels in train_label_encodings:\n",
        "  annotated_train_data[\"input_ids\"].append(train_text_encodings['input_ids'][position])\n",
        "  annotated_train_data[\"attention_mask\"].append(train_text_encodings['attention_mask'][position])\n",
        "  annotated_train_data[\"labels\"].append(labels)\n",
        "  position += 1\n",
        "position = 0\n",
        "for labels in test_label_encodings:\n",
        "  annotated_test_data[\"input_ids\"].append(test_text_encodings['input_ids'][position])\n",
        "  annotated_test_data[\"attention_mask\"].append(test_text_encodings['attention_mask'][position])\n",
        "  annotated_test_data[\"labels\"].append(labels)\n",
        "  position += 1\n",
        "\n",
        "features = datasets.Features({\n",
        "  \"input_ids\": datasets.Sequence(feature=datasets.Value(dtype=\"int32\")),\n",
        "  \"attention_mask\": datasets.Sequence(feature=datasets.Value(dtype=\"int8\")),\n",
        "  \"labels\": datasets.Sequence(feature=datasets.ClassLabel(num_classes=len(label2id), names=list(label2id)))\n",
        "})\n",
        "\n",
        "train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(annotated_train_data), features=features)\n",
        "test_dataset = datasets.Dataset.from_pandas(pd.DataFrame(annotated_test_data), features=features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the format and columns for the training and test datasets.**"
      ],
      "metadata": {
        "id": "vDgRKyptcQ_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FEYlMoLpz5P-"
      },
      "outputs": [],
      "source": [
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh_YZJRvNEfM"
      },
      "source": [
        "## Training & evaluation with CamemBERT and CamemBERTCRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpSE5LlySk84"
      },
      "source": [
        "### CamemBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model definition"
      ],
      "metadata": {
        "id": "F4KKjRt5dz7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set kernel to GPU.**"
      ],
      "metadata": {
        "id": "4tgOODIPc1I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "sHBtjltbc7qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the model for fine tuning.**"
      ],
      "metadata": {
        "id": "X-TFDFy1c7En"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrCWd6nxSqBL"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_scheduler, AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_id, \n",
        "                                                        num_labels=len(unique_labels),\n",
        "                                                        label2id=label2id,\n",
        "                                                        id2label=id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add model to GPU.**"
      ],
      "metadata": {
        "id": "fB6d6kDldF2M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVH5AQQLH-OQ"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set hyperparameters.**"
      ],
      "metadata": {
        "id": "lRiMvO1rdIsu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e78-MChyEEw0"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(\n",
        "  model.parameters(),\n",
        "  lr = 4e-5,\n",
        ")\n",
        "\n",
        "epochs = 80\n",
        "\n",
        "num_training_steps = epochs * len(train_dataset)\n",
        "print(epochs, len(train_dataset), num_training_steps)\n",
        "\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fine tuning"
      ],
      "metadata": {
        "id": "mbuZ_J3Sd3bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model (Fine tuning).**"
      ],
      "metadata": {
        "id": "y7CXVskVdL6-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt0ebZvGEJo8"
      },
      "outputs": [],
      "source": [
        "# Train phase\n",
        "train_loss_set = []\n",
        "sequence_length = 512\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  print(f'Epoch number = {epoch}')\n",
        "  # Train the model\n",
        "  model.train()\n",
        "  for step, batch in enumerate(train_dataset):\n",
        "    # if step % 400 == 0 and not step == 0:\n",
        "    #   print(f'  Step {step}  of {len(train_dataloader)}.')\n",
        "\n",
        "    # Add batch to device CPU or GPU\n",
        "    batch = tuple(t.to(device) for t in batch.values())\n",
        "    # Unpack the inputs from our dataloader\n",
        "    # b_labels, b_input_ids, b_token_type_ids, b_input_mask = batch\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # print(b_input_ids.shape, b_input_mask.shape, b_labels.shape)\n",
        "\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids.reshape(1, sequence_length), \n",
        "                    token_type_ids = None, \n",
        "                    attention_mask = b_input_mask.reshape(1, sequence_length), \n",
        "                    labels = b_labels.reshape(1, sequence_length)\n",
        "    )\n",
        "\n",
        "    # print(outputs)\n",
        "    # print(outputs[0].shape, outputs[1].shape)\n",
        "\n",
        "    # Get loss value\n",
        "    loss = outputs[0]\n",
        "    # print(loss)\n",
        "    # Add it to train loss list\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    lr_scheduler\n",
        "    progress_bar.update(1)\n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "  \n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test of the fine-tuned model"
      ],
      "metadata": {
        "id": "1tsRd55zd7GM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the fine-tuned model.**"
      ],
      "metadata": {
        "id": "jYvsYrp9dQFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOcA8rGP9aAr"
      },
      "outputs": [],
      "source": [
        "# Test Phase\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def evaluate(model, dataset, ner_labels):\n",
        "  cpt = 0\n",
        "  all_predictions = []\n",
        "  all_labels = []\n",
        "  nb_test_steps = 0\n",
        "  for batch in dataset:\n",
        "    # Add batch to device CPU or GPU\n",
        "    batch = tuple(t.to(device) for t in batch.values())\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs =  model(b_input_ids.reshape(1, sequence_length),\n",
        "                     token_type_ids = None, \n",
        "                     attention_mask = b_input_mask.reshape(1, sequence_length), \n",
        "                     )\n",
        "      logits = outputs[0]\n",
        "      # print(logits)\n",
        "    # Move logits and labels to CPU if GPU is used\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    labels = b_labels.reshape(1, sequence_length)\n",
        "    labels = labels.to('cpu').numpy()\n",
        "    \n",
        "    predictions = np.argmax(logits, axis = -1)\n",
        "    for prediction, label in zip(predictions, labels):\n",
        "      for predicted_idx, label_idx in zip(prediction, label):\n",
        "        if label_idx == -100:\n",
        "          cpt += 1\n",
        "          continue\n",
        "        # print(f\"Predicted_idx ner_labels[{predicted_idx}] = {ner_labels[predicted_idx]} / Label_idx ner_labels[{label_idx}] = {ner_labels[label_idx]}\")\n",
        "        all_predictions.append(ner_labels[predicted_idx])\n",
        "        all_labels.append(ner_labels[label_idx])\n",
        "    nb_test_steps += 1\n",
        "    # print(tokenizer.decode(b_input_ids[0]))\n",
        "  print(cpt, nb_test_steps)\n",
        "  print(len(all_predictions), len(all_labels))\n",
        "  # print(f'Predicted labels: {all_predictions}')\n",
        "  # print(f'Real labels ----: {all_labels}')\n",
        "  return metric.compute(predictions=[all_predictions], references=[all_labels], zero_division=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the model on the training data.**"
      ],
      "metadata": {
        "id": "9T2s_8zmddbD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co4h-hrvh5D9"
      },
      "outputs": [],
      "source": [
        "# Check results of train dataset\n",
        "import pprint\n",
        "\n",
        "model.eval()\n",
        "\n",
        "results = evaluate(model, train_dataset, ner_labels=unique_labels)\n",
        "pprint.pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the model on the test data.**"
      ],
      "metadata": {
        "id": "gSVL9EfjdjQA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vus22NUKELo6"
      },
      "outputs": [],
      "source": [
        "results = evaluate(model, test_dataset, ner_labels=unique_labels)\n",
        "pprint.pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the model"
      ],
      "metadata": {
        "id": "M-aFHWQdd_Or"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save fine tuned model.**"
      ],
      "metadata": {
        "id": "QA_YhlgDdlUy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZUdA5Enbu-C"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"./deft_examen_camembert\")\n",
        "model.save_pretrained(\"./deft_examen_camembert\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xj-eSTySZeT"
      },
      "source": [
        "## CamemBERT CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare packages"
      ],
      "metadata": {
        "id": "6y-bPulsfybi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install needed package to use CRF.**"
      ],
      "metadata": {
        "id": "JymQav2RfrDC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XJh9BBd1NpGr"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-crf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import needed packages.**"
      ],
      "metadata": {
        "id": "UmT6589xf1w7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwQKaSWaNMIu"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from transformers import AutoModel, CamembertModel, CamembertTokenizerFast\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "from torchcrf import CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model definition"
      ],
      "metadata": {
        "id": "OoiT2Mnef5ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine CamemBERT and CRF.**"
      ],
      "metadata": {
        "id": "NMU0eFDvf8Sq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F7rnaJgxbO_o"
      },
      "outputs": [],
      "source": [
        "class CamemBERTCRF(nn.Module):\n",
        "  def __init__(self, num_labels):\n",
        "    super(CamemBERTCRF, self).__init__()\n",
        "    \n",
        "    self.encoder = CamembertModel.from_pretrained(\"camembert-base\")\n",
        "    \n",
        "    self.config = self.encoder.config\n",
        "    self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "    self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "    self.crf = CRF(num_tags=num_labels, batch_first=True)\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      input_ids=None,\n",
        "      attention_mask=None,\n",
        "      token_type_ids=None,\n",
        "      position_ids=None,\n",
        "      head_mask=None,\n",
        "      inputs_embeds=None,\n",
        "      labels=None,\n",
        "      output_attentions=None,\n",
        "      output_hidden_states=None,\n",
        "  ):\n",
        "      r\"\"\"\n",
        "      labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "          Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "          1]``.\n",
        "      \"\"\"\n",
        "      outputs = self.encoder(\n",
        "          input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          token_type_ids=token_type_ids,\n",
        "          position_ids=position_ids,\n",
        "          head_mask=head_mask,\n",
        "          inputs_embeds=inputs_embeds,\n",
        "          output_attentions=output_attentions,\n",
        "          output_hidden_states=output_hidden_states,\n",
        "      )\n",
        "\n",
        "      sequence_output = outputs.last_hidden_state\n",
        "      sequence_output = self.dropout(sequence_output)\n",
        "      logits = self.classifier(sequence_output)\n",
        "      \n",
        "      loss = None\n",
        "      if labels is not None:\n",
        "          log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)\n",
        "          loss = 0 - log_likelihood\n",
        "      else:\n",
        "          tags = self.crf.decode(logits)\n",
        "      tags = torch.tensor(tags)\n",
        "\n",
        "      output = (tags,) + outputs[2:]\n",
        "      return ((loss,) + output) if loss is not None else output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set kernel to GPU.**"
      ],
      "metadata": {
        "id": "4EsmdDG8gI7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "UV2SoYGygQUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the model for fine tuning.**"
      ],
      "metadata": {
        "id": "vIozOno0gTmu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cpq7zkNoeBFy"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_scheduler, AutoModelForTokenClassification\n",
        "\n",
        "model = CamemBERTCRF(num_labels=len(unique_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add model to GPU.**"
      ],
      "metadata": {
        "id": "1-vp7to4ggV4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y87v3URyeJSY"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set hyperparameters.**"
      ],
      "metadata": {
        "id": "LG4HS21RghSI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMudKHgWeQOD"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(\n",
        "  model.parameters(),\n",
        "  lr = 4e-5,\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "num_training_steps = epochs * len(train_dataset)\n",
        "print(epochs, len(train_dataset), num_training_steps)\n",
        "\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fine tuning"
      ],
      "metadata": {
        "id": "SuOt46Ptg8ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model (Fine tuning).**"
      ],
      "metadata": {
        "id": "5NfkxpNug7l5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQE6QKQVeQw8"
      },
      "outputs": [],
      "source": [
        "# Train phase\n",
        "train_loss_set = []\n",
        "sequence_length = 256\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  print(f'Epoch number = {epoch}')\n",
        "  # Train the model\n",
        "  model.train()\n",
        "  for step, batch in enumerate(train_dataset):\n",
        "    # if step % 400 == 0 and not step == 0:\n",
        "    #   print(f'  Step {step}  of {len(train_dataloader)}.')\n",
        "\n",
        "    # Add batch to device CPU or GPU\n",
        "    batch = tuple(t.to(device) for t in batch.values())\n",
        "    # Unpack the inputs from our dataloader\n",
        "    # b_labels, b_input_ids, b_token_type_ids, b_input_mask = batch\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # print(b_input_ids.shape, b_input_mask.shape, b_labels.shape)\n",
        "\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids.reshape(1, sequence_length), \n",
        "                    token_type_ids = None, \n",
        "                    attention_mask = b_input_mask.reshape(1, sequence_length), \n",
        "                    labels = b_labels.reshape(1, sequence_length)\n",
        "    )\n",
        "\n",
        "    # print(outputs)\n",
        "    # print(outputs[0].shape, outputs[1].shape)\n",
        "\n",
        "    # Get loss value\n",
        "    loss = outputs[0]\n",
        "    # print(loss)\n",
        "    # Add it to train loss list\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    lr_scheduler\n",
        "    progress_bar.update(1)\n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "  \n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test of the fine-tuned model"
      ],
      "metadata": {
        "id": "RrM7asGBhCnp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo9gfPeRG_io"
      },
      "outputs": [],
      "source": [
        "# from transformers import BertTokenizerFast, Trainer, TrainingArguments\n",
        "# from sklearn.metrics import classification_report, f1_score\n",
        "# from transformers.trainer_utils import IntervalStrategy\n",
        "\n",
        "# def compute_metrics(pred):\n",
        "#     labels = pred.label_ids.flatten()\n",
        "#     preds = pred.predictions.flatten()\n",
        "#     f1 = f1_score(labels, preds, average='macro')\n",
        "#     print(classification_report(labels, preds))\n",
        "#     return {\n",
        "#         'f1': f1\n",
        "#     }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llh3vcQmHGIq"
      },
      "outputs": [],
      "source": [
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     num_train_epochs=5,\n",
        "#     # per_device_train_batch_size=32,\n",
        "#     # per_device_eval_batch_size=32,\n",
        "#     learning_rate=2e-5,\n",
        "#     warmup_steps=200,\n",
        "#     weight_decay=0.01,\n",
        "#     evaluation_strategy=\"epoch\",\n",
        "#     save_strategy=IntervalStrategy.EPOCH,\n",
        "#     logging_dir='./logs',\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ4rc9C_HIPV"
      },
      "outputs": [],
      "source": [
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=test_dataset\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RSLtEQbHKAI"
      },
      "outputs": [],
      "source": [
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVUmEMlqHLCj"
      },
      "outputs": [],
      "source": [
        "# print(trainer.evaluate())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-Xj-eSTySZeT",
        "TB-eXxKxISye"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}